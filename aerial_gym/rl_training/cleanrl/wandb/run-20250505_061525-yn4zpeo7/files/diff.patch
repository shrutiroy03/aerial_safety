diff --git a/aerial_gym/rl_training/cleanrl/sac_continuous_action.py b/aerial_gym/rl_training/cleanrl/sac_continuous_action.py
index 98f5f07..2d3ae7a 100644
--- a/aerial_gym/rl_training/cleanrl/sac_continuous_action.py
+++ b/aerial_gym/rl_training/cleanrl/sac_continuous_action.py
@@ -530,6 +530,7 @@ if __name__ == "__main__":
         dtype=torch.float,
     ).to(device)
     logprobs = torch.zeros((args.num_steps, args.num_envs), dtype=torch.float).to(device)
+    margins = torch.zeros((args.num_steps, args.num_envs), dtype=torch.float).to(device)
     rewards = torch.zeros((args.num_steps, args.num_envs), dtype=torch.float).to(device)
     safety = torch.zeros((args.num_steps, args.num_envs), dtype=torch.float).to(device)
     dones = torch.zeros((args.num_steps, args.num_envs), dtype=torch.float).to(device)
@@ -571,8 +572,9 @@ if __name__ == "__main__":
                 actions[step] = torch.tensor(action, dtype=torch.float32, device=device)
 
                 # TRY NOT TO MODIFY: execute the game and log data.
-                next_obs, rewards[step], next_done, info = envs.step(action)
-                safety[step] = info["safety_margin"]
+                next_obs, margins[step], next_done, info = envs.step(action)
+                print("info keys:", info.keys())
+                rewards[step], safety[step] = margins[step]
                 next_obs = next_obs["observations"]
                 if 0 <= step <= 2:
                     for idx, d in enumerate(next_done):
diff --git a/aerial_gym/rl_training/cleanrl/wandb/debug-internal.log b/aerial_gym/rl_training/cleanrl/wandb/debug-internal.log
index 0504610..6ad34c1 120000
--- a/aerial_gym/rl_training/cleanrl/wandb/debug-internal.log
+++ b/aerial_gym/rl_training/cleanrl/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250416_141843-2bhvh4fm/logs/debug-internal.log
\ No newline at end of file
+run-20250505_061525-yn4zpeo7/logs/debug-internal.log
\ No newline at end of file
diff --git a/aerial_gym/rl_training/cleanrl/wandb/debug.log b/aerial_gym/rl_training/cleanrl/wandb/debug.log
index 7d9a1aa..9c6e51e 120000
--- a/aerial_gym/rl_training/cleanrl/wandb/debug.log
+++ b/aerial_gym/rl_training/cleanrl/wandb/debug.log
@@ -1 +1 @@
-run-20250416_141843-2bhvh4fm/logs/debug.log
\ No newline at end of file
+run-20250505_061525-yn4zpeo7/logs/debug.log
\ No newline at end of file
diff --git a/aerial_gym/rl_training/cleanrl/wandb/latest-run b/aerial_gym/rl_training/cleanrl/wandb/latest-run
index 4ffae1c..4cc03e0 120000
--- a/aerial_gym/rl_training/cleanrl/wandb/latest-run
+++ b/aerial_gym/rl_training/cleanrl/wandb/latest-run
@@ -1 +1 @@
-run-20250416_141843-2bhvh4fm
\ No newline at end of file
+run-20250505_061525-yn4zpeo7
\ No newline at end of file
diff --git a/aerial_gym/rl_training/rl_games/runner.py b/aerial_gym/rl_training/rl_games/runner.py
index 855a3e5..4646e89 100644
--- a/aerial_gym/rl_training/rl_games/runner.py
+++ b/aerial_gym/rl_training/rl_games/runner.py
@@ -1,331 +1,79 @@
-import numpy as np
-import os
-import yaml
-
-
-import isaacgym
-
-
-from aerial_gym.registry.task_registry import task_registry
-from aerial_gym.utils.helpers import parse_arguments
-
-import gym
-from gym import spaces
-from argparse import Namespace
-
-from rl_games.common import env_configurations, vecenv
-
-import torch
-import distutils
+from distutils.util import strtobool
+import argparse, os, yaml
 
 os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
-# import warnings
-# warnings.filterwarnings("error")
-
-
-class ExtractObsWrapper(gym.Wrapper):
-    def __init__(self, env):
-        super().__init__(env)
-
-    def reset(self, **kwargs):
-        observations, *_ = super().reset(**kwargs)
-        return observations["observations"]
-
-    def step(self, action):
-        observations, rewards, terminated, truncated, infos = super().step(action)
-
-        dones = torch.where(
-            terminated | truncated,
-            torch.ones_like(terminated),
-            torch.zeros_like(terminated),
-        )
-
-        return (
-            observations["observations"],
-            rewards,
-            dones,
-            infos,
-        )
-
-
-class AERIALRLGPUEnv(vecenv.IVecEnv):
-    def __init__(self, config_name, num_actors, **kwargs):
-        self.env = env_configurations.configurations[config_name]["env_creator"](**kwargs)
-        self.env = ExtractObsWrapper(self.env)
-
-    def step(self, actions):
-        return self.env.step(actions)
-
-    def reset(self):
-        return self.env.reset()
-
-    def reset_done(self):
-        return self.env.reset_done()
-
-    def get_number_of_agents(self):
-        return self.env.get_number_of_agents()
-
-    def get_env_info(self):
-        info = {}
-        info["action_space"] = spaces.Box(
-            -np.ones(self.env.task_config.action_space_dim),
-            np.ones(self.env.task_config.action_space_dim),
-        )
-        info["observation_space"] = spaces.Box(
-            np.ones(self.env.task_config.observation_space_dim) * -np.Inf,
-            np.ones(self.env.task_config.observation_space_dim) * np.Inf,
-        )
-        print(info["action_space"], info["observation_space"])
-        return info
-
 
-env_configurations.register(
-    "position_setpoint_task",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task("position_setpoint_task", **kwargs),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
 
-env_configurations.register(
-    "position_setpoint_task_sim2real",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task(
-            "position_setpoint_task_sim2real", **kwargs
-        ),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-env_configurations.register(
-    "position_setpoint_task_acceleration_sim2real",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task(
-            "position_setpoint_task_acceleration_sim2real", **kwargs
-        ),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-env_configurations.register(
-    "navigation_task",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task("navigation_task", **kwargs),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-env_configurations.register(
-    "position_setpoint_task_reconfigurable",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task(
-            "position_setpoint_task_reconfigurable", **kwargs
-        ),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-env_configurations.register(
-    "position_setpoint_task_morphy",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task(
-            "position_setpoint_task_morphy", **kwargs
-        ),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-env_configurations.register(
-    "position_setpoint_task_sim2real_end_to_end",
-    {
-        "env_creator": lambda **kwargs: task_registry.make_task(
-            "position_setpoint_task_sim2real_end_to_end", **kwargs
-        ),
-        "vecenv_type": "AERIAL-RLGPU",
-    },
-)
-
-vecenv.register(
-    "AERIAL-RLGPU",
-    lambda config_name, num_actors, **kwargs: AERIALRLGPUEnv(config_name, num_actors, **kwargs),
-)
-
-
-def get_args():
-    from isaacgym import gymutil
-
-    custom_parameters = [
-        {
-            "name": "--seed",
-            "type": int,
-            "default": 0,
-            "required": False,
-            "help": "Random seed, if larger than 0 will overwrite the value in yaml config.",
-        },
-        {
-            "name": "--tf",
-            "required": False,
-            "help": "run tensorflow runner",
-            "action": "store_true",
-        },
-        {
-            "name": "--train",
-            "required": False,
-            "help": "train network",
-            "action": "store_true",
-        },
-        {
-            "name": "--play",
-            "required": False,
-            "help": "play(test) network",
-            "action": "store_true",
-        },
-        {
-            "name": "--checkpoint",
-            "type": str,
-            "required": False,
-            "help": "path to checkpoint",
-        },
-        {
-            "name": "--file",
-            "type": str,
-            "default": "ppo_aerial_quad.yaml",
-            "required": False,
-            "help": "path to config",
-        },
-        {
-            "name": "--num_envs",
-            "type": int,
-            "default": "1024",
-            "help": "Number of environments to create. Overrides config file if provided.",
-        },
-        {
-            "name": "--sigma",
-            "type": float,
-            "required": False,
-            "help": "sets new sigma value in case if 'fixed_sigma: True' in yaml config",
-        },
-        {
-            "name": "--track",
-            "action": "store_true",
-            "help": "if toggled, this experiment will be tracked with Weights and Biases",
-        },
-        {
-            "name": "--wandb-project-name",
-            "type": str,
-            "default": "rl_games",
-            "help": "the wandb's project name",
-        },
-        {
-            "name": "--wandb-entity",
-            "type": str,
-            "default": None,
-            "help": "the entity (team) of wandb's project",
-        },
-        {
-            "name": "--task",
-            "type": str,
-            "default": "navigation_task",
-            "help": "Override task from config file if provided.",
-        },
-        {
-            "name": "--experiment_name",
-            "type": str,
-            "help": "Name of the experiment to run or load. Overrides config file if provided.",
-        },
-        {
-            "name": "--headless",
-            "type": lambda x: bool(distutils.util.strtobool(x)),
-            "default": "False",
-            "help": "Force display off at all times",
-        },
-        {
-            "name": "--horovod",
-            "action": "store_true",
-            "default": False,
-            "help": "Use horovod for multi-gpu training",
-        },
-        {
-            "name": "--rl_device",
-            "type": str,
-            "default": "cuda:0",
-            "help": "Device used by the RL algorithm, (cpu, gpu, cuda:0, cuda:1 etc..)",
-        },
-        {
-            "name": "--use_warp",
-            "type": lambda x: bool(distutils.util.strtobool(x)),
-            "default": "True",
-            "help": "Choose whether to use warp or Isaac Gym rendering pipeline.",
-        },
-    ]
-
-    # parse arguments
-    args = parse_arguments(description="RL Policy", custom_parameters=custom_parameters)
-
-    # name allignment
-    args.sim_device_id = args.compute_device_id
-    args.sim_device = args.sim_device_type
-    if args.sim_device == "cuda":
-        args.sim_device += f":{args.sim_device_id}"
-    return args
-
-
-def update_config(config, args):
-
-    if args["task"] is not None:
-        config["params"]["config"]["env_name"] = args["task"]
-    if args["experiment_name"] is not None:
-        config["params"]["config"]["name"] = args["experiment_name"]
-    config["params"]["config"]["env_config"]["headless"] = args["headless"]
-    config["params"]["config"]["env_config"]["num_envs"] = args["num_envs"]
-    config["params"]["config"]["env_config"]["use_warp"] = args["use_warp"]
-    if args["num_envs"] > 0:
-        config["params"]["config"]["num_actors"] = args["num_envs"]
-        # config['params']['config']['num_envs'] = args['num_envs']
-        config["params"]["config"]["env_config"]["num_envs"] = args["num_envs"]
-    if args["seed"] > 0:
-        config["params"]["seed"] = args["seed"]
-        config["params"]["config"]["env_config"]["seed"] = args["seed"]
-
-    config["params"]["config"]["player"] = {"use_vecenv": True}
-    return config
-
-
-if __name__ == "__main__":
+if __name__ == '__main__':
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--seed", type=int, default=0, required=False, 
+                    help="random seed, if larger than 0 will overwrite the value in yaml config")
+    ap.add_argument("-tf", "--tf", required=False, help="run tensorflow runner", action='store_true')
+    ap.add_argument("-t", "--train", required=False, help="train network", action='store_true')
+    ap.add_argument("-p", "--play", required=False, help="play(test) network", action='store_true')
+    ap.add_argument("-c", "--checkpoint", required=False, help="path to checkpoint")
+    ap.add_argument("-f", "--file", required=True, help="path to config")
+    ap.add_argument("-na", "--num_actors", type=int, default=0, required=False,
+                    help="number of envs running in parallel, if larger than 0 will overwrite the value in yaml config")
+    ap.add_argument("-s", "--sigma", type=float, required=False, help="sets new sigma value in case if 'fixed_sigma: True' in yaml config")
+    ap.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, this experiment will be tracked with Weights and Biases")
+    ap.add_argument("--wandb-project-name", type=str, default="rl_games",
+        help="the wandb's project name")
+    ap.add_argument("--wandb-entity", type=str, default=None,
+        help="the entity (team) of wandb's project")
     os.makedirs("nn", exist_ok=True)
     os.makedirs("runs", exist_ok=True)
 
-    args = vars(get_args())
-
-    config_name = args["file"]
+    args = vars(ap.parse_args())
+    config_name = args['file']
 
-    print("Loading config: ", config_name)
-    with open(config_name, "r") as stream:
+    print('Loading config: ', config_name)
+    with open(config_name, 'r') as stream:
         config = yaml.safe_load(stream)
 
-        config = update_config(config, args)
+        if args['num_actors'] > 0:
+            config['params']['config']['num_actors'] = args['num_actors']
+
+        if args['seed'] > 0:
+            config['params']['seed'] = args['seed']
+            config['params']['config']['env_config']['seed'] = args['seed']
 
         from rl_games.torch_runner import Runner
 
+        try:
+            import ray
+        except ImportError:
+            pass
+        else:
+            ray.init(object_store_memory=1024*1024*1000)
+
         runner = Runner()
         try:
             runner.load(config)
         except yaml.YAMLError as exc:
             print(exc)
 
-    rank = int(os.getenv("LOCAL_RANK", "0"))
-    if args["track"] and rank == 0:
+    global_rank = int(os.getenv("RANK", "0"))
+    if args["track"] and global_rank == 0:
         import wandb
-
         wandb.init(
             project=args["wandb_project_name"],
-            entity='shrutiroy03-princeton-university',
+            entity=args["wandb_entity"],
             sync_tensorboard=True,
             config=config,
             monitor_gym=True,
             save_code=True,
         )
+
     runner.run(args)
 
-    if args["track"] and rank == 0:
+    try:
+        import ray
+    except ImportError:
+        pass
+    else:
+        ray.shutdown()
+
+    if args["track"] and global_rank == 0:
         wandb.finish()
diff --git a/aerial_gym/rl_training/rl_games/sac_aerial_quad_navigation.yaml b/aerial_gym/rl_training/rl_games/sac_aerial_quad_navigation.yaml
index aaa450f..3d886af 100644
--- a/aerial_gym/rl_training/rl_games/sac_aerial_quad_navigation.yaml
+++ b/aerial_gym/rl_training/rl_games/sac_aerial_quad_navigation.yaml
@@ -4,12 +4,12 @@ params:
     name: sac #edit algo to sac
 
   model:
-    name: continuous_a2c_logstd
+    name: soft_actor_critic
   
   load_checkpoint: False
 
   network:
-    name: actor_critic
+    name: soft_actor_critic
     separate: False
     space:
       continuous:
@@ -24,7 +24,7 @@ params:
     mlp:
       units: [256,128,64]
       d2rl: False
-      activation: elu
+      activation: relu
       initializer:
         name: default
         scale: 2
diff --git a/aerial_gym/task/reach_avoid_task/__pycache__/reach_avoid_task.cpython-38.pyc b/aerial_gym/task/reach_avoid_task/__pycache__/reach_avoid_task.cpython-38.pyc
index d365c2e..dd968ad 100644
Binary files a/aerial_gym/task/reach_avoid_task/__pycache__/reach_avoid_task.cpython-38.pyc and b/aerial_gym/task/reach_avoid_task/__pycache__/reach_avoid_task.cpython-38.pyc differ
diff --git a/aerial_gym/task/reach_avoid_task/reach_avoid_task.py b/aerial_gym/task/reach_avoid_task/reach_avoid_task.py
index bf83947..3389699 100644
--- a/aerial_gym/task/reach_avoid_task/reach_avoid_task.py
+++ b/aerial_gym/task/reach_avoid_task/reach_avoid_task.py
@@ -304,7 +304,7 @@ class ReachAvoidTask(BaseTask):
         # This step must be done since the reset is done after the reward is calculated.
         # This enables the robot to send back an updated state, and an updated observation to the RL agent after the reset.
         # This is important for the RL agent to get the correct state after the reset.
-        self.rewards[:], self.safety_margin[:], self.terminations[:] = self.compute_rewards_and_crashes(self.obs_dict)
+        self.rewards[:], self.terminations[:] = self.compute_rewards_and_crashes(self.obs_dict)
 
         # logger.info(f"Curriculum Level: {self.curriculum_level}")
 
@@ -320,7 +320,8 @@ class ReachAvoidTask(BaseTask):
         # edit success logic
         # successes are are the sum of the environments which are to be truncated and have reached the target within a distance threshold
         successes = self.truncations * (
-            torch.norm(self.target_position - self.obs_dict["robot_position"], dim=1) < 1.0
+            #torch.norm(self.target_position - self.obs_dict["robot_position"], dim=1) < 1.0
+            self.rewards[:,0] > 0
         )
         successes = torch.where(self.terminations > 0, torch.zeros_like(successes), successes)
         timeouts = torch.where(
@@ -333,7 +334,7 @@ class ReachAvoidTask(BaseTask):
         self.infos["successes"] = successes
         self.infos["timeouts"] = timeouts
         self.infos["crashes"] = self.terminations
-        self.infos["safety_margin"] = self.safety_margin
+        # self.infos["safety_margin"] = safety_margin
 
         self.logging_sanity_check(self.infos)
         self.check_and_update_curriculum_level(
@@ -444,26 +445,62 @@ def compute_reward(
     curriculum_progress_fraction,
     parameter_dict
 ):
-    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor, Tensor] 
-    # edit reward (l(x)) computation
+    print("image_obs shape: ", image_obs.shape)
     min_wall_dist = torch.min(torch.cat([robot_pos, 1 - robot_pos], 1), 1)[0]
-    reward = torch.min(torch.cat(((parameter_dict["velocity_max"] - torch.abs(robot_linvel[:, 0])).unsqueeze(1),
-                                   (parameter_dict["velocity_max"] - torch.abs(robot_linvel[:, 1])).unsqueeze(1),
-                                   (parameter_dict["velocity_max"] - torch.abs(robot_linvel[:, 2])).unsqueeze(1),
-                                   (parameter_dict["angvel_max"] - torch.abs(angular_velocity[:, 0])).unsqueeze(1),
-                                   (parameter_dict["angvel_max"] - torch.abs(angular_velocity[:, 1])).unsqueeze(1),
-                                   (parameter_dict["angvel_max"] - torch.abs(angular_velocity[:, 2])).unsqueeze(1),
-                                   (parameter_dict["angle_max"] - torch.abs(euler_angles[:, 0])).unsqueeze(1),
-                                   (parameter_dict["angle_max"] - torch.abs(euler_angles[:, 1])).unsqueeze(1),
-                                   (parameter_dict["obs_dist_lmin"] - torch.amin(image_obs, [1,2])).unsqueeze(1),
-                                   (parameter_dict["wall_dist_lmin"] - min_wall_dist).unsqueeze(1)), 1), 1)[0]
+    min_obs_dist = torch.amin(image_obs, dim=(2, 3)).squeeze(1)
+    
+    velocity_max = torch.tensor(parameter_dict["velocity_max"], device=robot_linvel.device)
+    angvel_max = torch.tensor(parameter_dict["angvel_max"], device=angular_velocity.device)
+    angle_max = torch.tensor(parameter_dict["angle_max"], device=euler_angles.device)
+    obs_lmin = torch.tensor(parameter_dict["obs_dist_lmin"], device=robot_linvel.device)
+    wall_lmin = torch.tensor(parameter_dict["wall_dist_lmin"], device=min_wall_dist.device)
+
+
+    obs_gmin = torch.tensor(parameter_dict["obs_dist_gmin"], device=robot_linvel.device)
+    wall_gmin = torch.tensor(parameter_dict["wall_dist_gmin"], device=min_wall_dist.device)
+
+    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tuple[Tensor, Tensor], Tensor] 
+    # edit reward (l(x)) computation
+    # vel_diff = velocity_max - torch.abs(robot_linvel)
+    # angvel_diff = angvel_max - torch.abs(angular_velocity)
+    # angle_diff = angle_max - torch.abs(euler_angles[:, ])
+    # vel_min = torch.min(vel_diff, dim=1)[0]
+
+    # reward = torch.min(torch.cat(((velocity_max - torch.abs(robot_linvel[:, 0])).unsqueeze(1),
+    #                                (velocity_max - torch.abs(robot_linvel[:, 1])).unsqueeze(1),
+    #                                (velocity_max - torch.abs(robot_linvel[:, 2])).unsqueeze(1),
+    #                                (angvel_max - torch.abs(angular_velocity[:, 0])).unsqueeze(1),
+    #                                (angvel_max - torch.abs(angular_velocity[:, 1])).unsqueeze(1),
+    #                                (angvel_max - torch.abs(angular_velocity[:, 2])).unsqueeze(1),
+    #                                (angle_max - torch.abs(euler_angles[:, 0])).unsqueeze(1),
+    #                                (angle_max - torch.abs(euler_angles[:, 1])).unsqueeze(1),
+    #                                (obs_lmin - torch.amin(image_obs, [1,2])).unsqueeze(1),
+    #                                (wall_lmin - min_wall_dist).unsqueeze(1)), 1), 1)[0]
+    abs_metrics = torch.stack([velocity_max - torch.abs(robot_linvel[:, 0]),
+                                velocity_max - torch.abs(robot_linvel[:, 1]),
+                                velocity_max - torch.abs(robot_linvel[:, 2]),
+                                angvel_max - torch.abs(angular_velocity[:, 0]),
+                                angvel_max - torch.abs(angular_velocity[:, 1]),
+                                angvel_max - torch.abs(angular_velocity[:, 2]),
+                                angle_max - torch.abs(euler_angles[:, 0]),
+                                angle_max - torch.abs(euler_angles[:, 1]),
+                                obs_lmin - min_obs_dist,
+                                wall_lmin - min_wall_dist
+                              ], dim=1)  # shape: [num_envs, 10]
+    reward = torch.min(abs_metrics, dim=1)[0]
     
-    g_x = torch.min(torch.cat(((parameter_dict["obs_dist_gmin"] - torch.amin(image_obs, [1,2])).unsqueeze(1),
-                                (parameter_dict["wall_dist_gmin"] - min_wall_dist).unsqueeze(1)), 1), 1)[0]
+    # g_x = torch.min(torch.cat(((obs_gmin - torch.amin(image_obs, [1,2])).unsqueeze(1),
+    #                             (wall_gmin - min_wall_dist).unsqueeze(1)), 1), 1)[0]
+
+    g_x_metrics = torch.stack([obs_gmin - min_obs_dist,  # shape: [num_envs]
+                                wall_gmin - min_wall_dist                      # shape: [num_envs]
+                              ], dim=1)  # shape: [num_envs, 2]
+    g_x = torch.min(g_x_metrics, dim=1)[0]  # shape: [num_envs]
+
 
     reward[:] = torch.where(
         crashes > 0,
         parameter_dict["collision_penalty"] * torch.ones_like(reward),
         reward,
     )
-    return reward, g_x, crashes
+    return (reward, g_x), crashes
